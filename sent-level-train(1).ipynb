{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport torch\nfrom torch.nn import BCEWithLogitsLoss, BCELoss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\nimport pickle\nfrom transformers import *\nfrom tqdm import tqdm, trange\nfrom ast import literal_eval\nfrom sklearn import metrics\nimport random\nimport matplotlib.pyplot as plt\nfrom numba import jit, cuda\nfrom transformers import BertTokenizer\nfrom sklearn.metrics import multilabel_confusion_matrix\nimport pickle\n#import cuda\n\n\ndef hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n    '''\n    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n    http://stackoverflow.com/q/32239577/395857\n    '''\n    acc_list = []\n    for i in range(y_true.shape[0]):\n        set_true = set( np.where(y_true[i])[0] )\n        set_pred = set( np.where(y_pred[i])[0] )\n        #print('\\nset_true: {0}'.format(set_true))\n        #print('set_pred: {0}'.format(set_pred))\n        tmp_a = None\n        if len(set_true) == 0 and len(set_pred) == 0:\n            tmp_a = 1\n        else:\n            tmp_a = len(set_true.intersection(set_pred))/\\\n                    float( len(set_true.union(set_pred)) )\n        #print('tmp_a: {0}'.format(tmp_a))\n        acc_list.append(tmp_a)\n    return np.mean(acc_list)\n\n\n#using cpu as our device\ndevice = torch.device(\"cuda\")\n# If there's a GPU available...\nprint(torch.cuda.is_available())\n\n\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 5\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed(seed_val)\n\n#pd.set_option('display.max_columns', None)\n\n\n# Load the dataset into a pandas dataframe.\ndff = pd.read_csv(\"../input/frames-sentence-level/actual_dataset.csv\", header=None, names=['text','code', 'one_hot_labels'])\n#print(dff)\n\n#train_test_validate split (60,20,20)\ntrain, validate, test = \\\n              np.split(dff.sample(frac=1, random_state=42),\n                       [int(.6*len(dff)), int(.8*len(dff))])\n\n#print(train)\n#print(validate)\n#print(test)\ntrain_validate_set = [train,validate]\ndf = pd.concat(train_validate_set,ignore_index='true')\ndf = pd.DataFrame(df, columns=['text', 'code', 'one_hot_labels'])\n#df = dff\n#print(df)\n#pd.set_option('display.max_rows', None)\n#print(test['code'].value_counts().nlargest(10)[test['code'].value_counts().nlargest(10) ==test['code'].value_counts().nlargest(10)])\nnum_labels = 15\n#print('Unique comments: ', df.text.nunique() == df.shape[0])\n'''\nword_len_list = []\n#print('average sentence length: ', df.text.str.split().str.len().mean())\nlength = df.text.str.split().str.len()\n\nfor i in length:\n    print(i)\n    word_len_list.append(i)\nplt.hist(word_len_list, bins=500)\nplt.xlabel('Number of Words')\nplt.xticks(np.arange(0,200, step=5))\nplt.margins(0)\nplt.ylabel('Number of Sentences')\nplt.title('Distribution of Words')\nplt.show()\n#print('stdev sentence length: ', df.text.str.split().str.len().std())\n#print('Null values: ', df.isnull().values.any())\n'''\n#df[\"text\"] = df[\"text\"].apply(lambda x: list(map(str, x)))\n#df[\"one_hot_labels\"] = df[\"one_hot_labels\"]\nlabels = list(df.one_hot_labels.values)\n\n\n#Rearranging the label data in the dataframe to feed in the bert encodings\nlabels_re = []\nlabels_re_re = []\ntemp = []\nfor a in labels:\n    labels_re = []\n    for aa in a:\n\n        if aa != \" \":\n            #print(aa)\n            #print('###')\n            labels_re.append(int(aa))\n            #print(labels_re)\n    labels_re_re.append(labels_re)\n    temp = labels_re_re\n    #print(labels_re_re)\n#print(temp)\n\nlabels = temp\ndf['one_hot_labels'] = labels\nnew_df = df[['text', 'one_hot_labels']]\n\ndf = new_df\n#print(df)\n\nlabels = list(df.one_hot_labels.values)\ncomments = list(df.text.values)\n\n\nmax_length = 128\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # tokenizer\nencodings = tokenizer.batch_encode_plus(comments,max_length=max_length,pad_to_max_length=True) # tokenizer's encoding method\n#print('tokenizer outputs: ', encodings.keys())\n\ninput_ids = encodings['input_ids'] # tokenized and encoded sentences\nprint('input_ids', len(input_ids))\ntoken_type_ids = encodings['token_type_ids'] # token type ids\nattention_masks = encodings['attention_mask'] # attention masks\n\n# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later\nlabel_counts = df.one_hot_labels.astype(str).value_counts()\n#print('label counts', label_counts)\none_freq = label_counts[label_counts==1].keys()\n#print('one freq', one_freq)\none_freq_idxs = sorted(list(df[df.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\n#print('df label indices with only one instance: ', one_freq_idxs)\n\n# Gathering single instance inputs to force into the training set after stratified split\none_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n#print(one_freq_input_ids)\none_freq_token_types = [token_type_ids.pop(i) for i in one_freq_idxs]\none_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\none_freq_labels = [labels.pop(i) for i in one_freq_idxs]\n\n# Use train_test_split to split our data into train and validation sets\n\ntrain_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels, token_type_ids,attention_masks, test_size=0.25, shuffle = False) \ntrain_inputs.extend(one_freq_input_ids)\ntrain_labels.extend(one_freq_labels)\ntrain_masks.extend(one_freq_attention_masks)\ntrain_token_types.extend(one_freq_token_types)\n\n#print(train_labels.value_counts().nsmallest(10)[train_labels.value_counts().nsmallest(10) == train_labels.value_counts().nsmallest(10)])\n\n\n#print('train labels', len(train_labels))\n#print('val labels', len(validation_labels))\n#print('plain train inputs',train_inputs)\n#print('plain train labels', train_labels)\n\n# Convert all of our data into torch tensors, the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)\ntrain_token_types = torch.tensor(train_token_types)\n\n#print('tensor train inputs', train_inputs)\n#print('tensor train labels', train_labels)\n\nvalidation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)\nvalidation_token_types = torch.tensor(validation_token_types)\n\n# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\ntorch.save(validation_dataloader,'validation_data_loader')\ntorch.save(train_dataloader,'train_data_loader')\n\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\nmodel.cuda()\n\n# setting custom optimization parameters. You may implement a scheduler here as well.\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n#lr = np.arange(2e-5, 6e-5, 0.000002)\n#lr = list(lr)\n#learning_rate_list = []\n#loss_list=[]\n#print('lr', len(lr))\n\n#learning_rate_list.append(l)\n\noptimizer = AdamW(optimizer_grouped_parameters,lr=5.8e-5,correct_bias=True)\n#optimizer = AdamW(model.parameters(),lr=2e-5)  # Default optimization\n\n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\n#Number of training epochs (authors recommend between 2 and 4)\nepochs = 3\n\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n\n    # Training\n\n    # Set our model to training mode (as opposed to evaluation mode)\n    model.train()\n\n    # Tracking variables\n    tr_loss = 0  # running loss\n    nb_tr_examples, nb_tr_steps = 0, 0\n\n    # Train the data for one epoch\n    for step, batch in enumerate(train_dataloader):\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels, b_token_types = batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n\n        # Forward pass for multilabel classification\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n        #print('outputs', outputs)\n        logits = outputs[0]\n\n        loss_func = BCEWithLogitsLoss()\n        loss = loss_func(logits.view(-1, num_labels),\n                         b_labels.type_as(logits).view(-1, num_labels))    # convert labels to float for calculation\n\n        # Backward pass\n        loss.backward()\n\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n\n        # Update tracking variables\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n\n    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n\n\n\n    ###############################################################################\n\n    # Validation\n\n    # Put model in evaluation mode to evaluate loss on the validation set\n    model.eval()\n\n    # Variables to gather full output\n    logit_preds, true_labels, pred_labels, tokenized_texts = [], [], [], []\n\n    # Predict\n    for i, batch in enumerate(validation_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels, b_token_types = batch\n        with torch.no_grad():\n            # Forward pass\n            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n            b_logit_pred = outs[0]\n            pred_label = torch.sigmoid(b_logit_pred)\n\n            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n            pred_label = pred_label.to('cpu').numpy()\n            b_labels = b_labels.to('cpu').numpy()\n\n        tokenized_texts.append(b_input_ids)\n        logit_preds.append(b_logit_pred)\n        true_labels.append(b_labels)\n        pred_labels.append(pred_label)\n\n    # Flatten outputs\n    pred_labels = [item for sublist in pred_labels for item in sublist]\n    #print(pred_labels)\n    true_labels = [item for sublist in true_labels for item in sublist]\n    #print(true_labels)\n    # Calculate Accuracy\n    threshold = 0.50\n    pred_bools = [pl > threshold for pl in pred_labels]\n    #print(pred_bools)\n    true_bools = [tl == 1 for tl in true_labels]\n    #print(true_bools)\n    val_f1_accuracy = f1_score(true_bools, pred_bools, average='micro') * 100\n    val_flat_accuracy = accuracy_score(true_bools, pred_bools) * 100\n\n    print('F1 Validation Accuracy: ', val_f1_accuracy)\n    print('Flat Validation Accuracy: ', val_flat_accuracy)\n\npickle.dump(model, open('model_pickle.sav', 'wb'))\n\n\n#test begins\n\nprint(test)\n# Gathering input data\ntest_labels = list(test.one_hot_labels.values)\n\n#Rearranging the label data in the dataframe to feed in the bert encodings\ntest_labels_re = []\ntest_labels_re_re = []\ntest_temp = []\nfor a in test_labels:\n    test_labels_re = []\n    for aa in a:\n\n        if aa != \" \":\n            #print(aa)\n            #print('###')\n            test_labels_re.append(int(aa))\n            #print(labels_re)\n    test_labels_re_re.append(test_labels_re)\n    test_temp = test_labels_re_re\n    #print(labels_re_re)\n#print(temp)\n\ntest_labels = test_temp\ntest['one_hot_labels'] = test_labels\nnew_test = test[['text', 'one_hot_labels']]\nprint(new_test)\nnew_test = test\n\ntest_labels = list(test.one_hot_labels.values)\ntest_comments = list(test.text.values)\n\n\n\n# Encoding input data\ntest_encodings = tokenizer.batch_encode_plus(test_comments,max_length=max_length,pad_to_max_length=True, truncation=True)\ntest_input_ids = test_encodings['input_ids']\nprint('test_input_ids', len(test_input_ids))\ntest_token_type_ids = test_encodings['token_type_ids']\ntest_attention_masks = test_encodings['attention_mask']\n\n# Make tensors out of data\ntest_inputs = torch.tensor(test_input_ids)\ntest_labels = torch.tensor(test_labels)\ntest_masks = torch.tensor(test_attention_masks)\ntest_token_types = torch.tensor(test_token_type_ids)\n# Create test dataloader\ntest_data = TensorDataset(test_inputs, test_masks, test_labels, test_token_types)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n# Save test dataloader\ntorch.save(test_dataloader,'test_data_loader')\n\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport torch\nfrom torch.nn import BCEWithLogitsLoss, BCELoss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score, ConfusionMatrixDisplay\nimport pickle\nfrom transformers import *\nfrom tqdm import tqdm, trange\nfrom ast import literal_eval\nfrom sklearn import metrics\nimport random\nimport matplotlib.pyplot as plt\nfrom numba import jit, cuda\nfrom transformers import BertTokenizer\nfrom sklearn.metrics import multilabel_confusion_matrix\nimport pickle\nfrom operator import and_\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy import interp\nfrom itertools import cycle\nfrom PIL import Image  \nimport PIL\n\n\ndef hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n    '''\n    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n    http://stackoverflow.com/q/32239577/395857\n    '''\n    acc_list = []\n    for i in range(y_true.shape[0]):\n        set_true = set( np.where(y_true[i])[0] )\n        set_pred = set( np.where(y_pred[i])[0] )\n        #print('\\nset_true: {0}'.format(set_true))\n        #print('set_pred: {0}'.format(set_pred))\n        tmp_a = None\n        if len(set_true) == 0 and len(set_pred) == 0:\n            tmp_a = 1\n        else:\n            tmp_a = len(set_true.intersection(set_pred))/\\\n                    float( len(set_true.union(set_pred)) )\n        #print('tmp_a: {0}'.format(tmp_a))\n        acc_list.append(tmp_a)\n    return np.mean(acc_list)\n\n#load the model from disk\nmodel = pickle.load(open(\"../input/model/model_pickle.sav\", 'rb'))\n\n# Load the dataset into a pandas dataframe.\ndff = pd.read_csv('../input/frames-sentence-level/actual_dataset.csv', header=None, names=['text','code', 'one_hot_labels'])\n#print(dff)\n\n#train_test_validate split (60,20,20)\ntrain, validate, test = \\\n              np.split(dff.sample(frac=1, random_state=42),\n                       [int(.6*len(dff)), int(.8*len(dff))])\n\n\n\n\n#using cpu as our device\ndevice = torch.device(\"cuda\")\n\n#test begins\n\n# Gathering input data\ntest_labels = list(test.one_hot_labels.values)\n\n#Rearranging the label data in the dataframe to feed in the bert encodings\ntest_labels_re = []\ntest_labels_re_re = []\ntest_temp = []\nfor a in test_labels:\n    test_labels_re = []\n    for aa in a:\n\n        if aa != \" \":\n            #print(aa)\n            #print('###')\n            test_labels_re.append(int(aa))\n            #print(labels_re)\n    test_labels_re_re.append(test_labels_re)\n    test_temp = test_labels_re_re\n    #print(labels_re_re)\n#print(temp)\n\ntest_labels = test_temp\ntest['one_hot_labels'] = test_labels\nnew_test = test[['text', 'one_hot_labels']]\n#print(new_test)\ntest = new_test\n\ntest_labels = list(test.one_hot_labels.values)\ntest_comments = list(test.text.values)\n\n\n#count_label = tf.reduce_sum(np.array(test_labels), axis=0)\n#print(count_label)\n\n\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # tokenizer\n\n\n# Encoding input data\ntest_encodings = tokenizer.batch_encode_plus(test_comments,max_length=128,pad_to_max_length=True, truncation=True)\ntest_input_ids = test_encodings['input_ids']\n#print('test_input_ids', len(test_input_ids))\ntest_token_type_ids = test_encodings['token_type_ids']\ntest_attention_masks = test_encodings['attention_mask']\n\n# Make tensors out of data\ntest_inputs = torch.tensor(test_input_ids)\ntest_labels = torch.tensor(test_labels)\ntest_masks = torch.tensor(test_attention_masks)\ntest_token_types = torch.tensor(test_token_type_ids)\n# Create test dataloader\ntest_data = TensorDataset(test_inputs, test_masks, test_labels, test_token_types)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)\n# Save test dataloader\n#torch.save(test_dataloader,'test_data_loader')\n#Test\ndf = test\n\n#Put model in evaluation mode to evaluate loss on the validation set\nmodel.eval()\n\n#track variables\nlogit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n\n# Predict\nfor i, batch in enumerate(test_dataloader):\n  batch = tuple(t.to(device) for t in batch)\n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask, b_labels, b_token_types = batch\n  with torch.no_grad():\n    # Forward pass\n    outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    b_logit_pred = outs[0]\n    pred_label = torch.sigmoid(b_logit_pred)\n\n    b_logit_pred = b_logit_pred.detach().cpu().numpy()\n    pred_label = pred_label.to('cpu').numpy()\n    b_labels = b_labels.to('cpu').numpy()\n\n  tokenized_texts.append(b_input_ids)\n  logit_preds.append(b_logit_pred)\n  true_labels.append(b_labels)\n  pred_labels.append(pred_label)\n\n# Flatten outputs\ntokenized_texts = [item for sublist in tokenized_texts for item in sublist]\npred_labels = [item for sublist in pred_labels for item in sublist]\ntrue_labels = [item for sublist in true_labels for item in sublist]\n# Converting flattened binary values to boolean values\ntrue_bools = [tl==1 for tl in true_labels]\n\npred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option(\"display.max_colwidth\", -1)\n\n\ntrue_bools_wrong , pred_bools_wrong = [],[]\n\nfor t in range(len(true_bools)):\n    if str(true_bools[t]) != str(pred_bools[t]):\n        #print(df.iloc[t])\n        #print(pred_bools[t])\n        true_bools_wrong.append(true_bools[t])\n        pred_bools_wrong.append(pred_bools[t])\n\n#print('Test F1 Accuracy: ', f1_score(true_bools_wrong, pred_bools_wrong,average='micro'))\n#print('Test Flat Accuracy: ', accuracy_score(true_bools_wrong, pred_bools_wrong),'\\n')\n#print('Hamming score: {0}'.format(hamming_score(np.array(true_bools_wrong), np.array(pred_bools_wrong))))  \n\n#clf_report = classification_report(true_bools,pred_bools,target_names=['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14'])\n#pickle.dump(clf_report, open('classification_report.txt','wb')) #save report\nprint(multilabel_confusion_matrix(true_bools_wrong,pred_bools_wrong))\n#print(clf_report)  \n\ny = label_binarize(true_bools, classes=['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14'])\nn_classes = y.shape[1]\n\nz = label_binarize(pred_bools, classes=['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14'])\n\n\nprint(y)\nprint(z)\n#print(true_bools)\n#print(pred_bools)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y[:,i], z[:,i], pos_label = 15)\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n#print(roc_auc)\nprint(fpr)\nprint('##########################')\nprint(tpr)\n'''\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y.ravel(), z.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n#Compute macro-average ROC curve and ROC area\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure(figsize=(12,8))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['#00FFFF','#C0C0C0','#0000FF','#808080','#0000A0','#000000','#ADD8E6','#FFA500','#800080','#A52A2A','#FFFF00','#800000','#00FF00','#008000','#FF00FF'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\nplt.savefig('ROC.jpg')\n   \n\n\n                \n# Print and save classification report\nprint('Test F1 Accuracy: ', f1_score(true_bools, pred_bools,average='micro'))\nprint('Test F1 Macro:',  f1_score(true_bools, pred_bools,average='macro'))\nprint('Test Flat Accuracy: ', accuracy_score(true_bools, pred_bools),'\\n')\nprint('Hamming score: {0}'.format(hamming_score(np.array(true_bools), np.array(pred_bools))))  \n\nclf_report = classification_report(true_bools,pred_bools,target_names=['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14'])\n#pickle.dump(clf_report, open('classification_report.txt','wb')) #save report\nprint(multilabel_confusion_matrix(true_bools,pred_bools))\n#print(clf_report)\n\n\n'''\n\n\n","execution_count":6,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:169: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","name":"stderr"},{"output_type":"stream","text":"[[[12930  1169]\n  [ 1366  1594]]\n\n [[16318   164]\n  [  464   113]]\n\n [[15469   414]\n  [  709   467]]\n\n [[15422   408]\n  [  761   468]]\n\n [[10884  1614]\n  [ 2036  2525]]\n\n [[12332  1093]\n  [ 2485  1149]]\n\n [[14619   697]\n  [  944   799]]\n\n [[15847   424]\n  [  405   383]]\n\n [[14756   465]\n  [ 1135   703]]\n\n [[13448  1186]\n  [ 1665   760]]\n\n [[13548   890]\n  [ 1982   639]]\n\n [[14180   923]\n  [ 1193   763]]\n\n [[11136  1572]\n  [ 1879  2472]]\n\n [[16556   142]\n  [  280    81]]\n\n [[16714   108]\n  [  216    21]]]\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 1 0 0]\n [0 0 0 ... 1 0 0]\n ...\n [1 0 0 ... 0 0 0]\n [0 0 0 ... 0 1 0]\n [0 0 0 ... 0 0 0]]\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 1 0 0]\n [0 0 0 ... 1 0 0]\n ...\n [1 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n{0: array([0.        , 0.15715712, 1.        ]), 1: array([0.        , 0.01427356, 1.        ]), 2: array([0.        , 0.03901194, 1.        ]), 3: array([0.        , 0.07214436, 1.        ]), 4: array([0.        , 0.09625411, 1.        ]), 5: array([0.        , 0.08797101, 1.        ]), 6: array([0.        , 0.08357061, 1.        ]), 7: array([0.        , 0.24154125, 1.        ]), 8: array([0.        , 0.01172207, 1.        ]), 9: array([0.        , 0.00802426, 1.        ]), 10: array([0.        , 0.04903302, 1.        ]), 11: array([0.        , 0.04289465, 1.        ]), 12: array([0.        , 0.24509115, 1.        ]), 13: array([0.       , 0.1077913, 1.       ]), 14: array([0.        , 0.08567836, 1.        ])}\n##########################\n{0: array([nan, nan, nan]), 1: array([nan, nan, nan]), 2: array([nan, nan, nan]), 3: array([nan, nan, nan]), 4: array([nan, nan, nan]), 5: array([nan, nan, nan]), 6: array([nan, nan, nan]), 7: array([nan, nan, nan]), 8: array([nan, nan, nan]), 9: array([nan, nan, nan]), 10: array([nan, nan, nan]), 11: array([nan, nan, nan]), 12: array([nan, nan, nan]), 13: array([nan, nan, nan]), 14: array([nan, nan, nan])}\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:813: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n  UndefinedMetricWarning)\n","name":"stderr"},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"'\\n# Compute micro-average ROC curve and ROC area\\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y.ravel(), z.ravel())\\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\\n\\n#Compute macro-average ROC curve and ROC area\\n\\n# First aggregate all false positive rates\\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\\n\\n# Then interpolate all ROC curves at this points\\nmean_tpr = np.zeros_like(all_fpr)\\nfor i in range(n_classes):\\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\\n\\n# Finally average it and compute AUC\\nmean_tpr /= n_classes\\n\\nfpr[\"macro\"] = all_fpr\\ntpr[\"macro\"] = mean_tpr\\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\\n\\n# Plot all ROC curves\\nplt.figure(figsize=(12,8))\\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\\n         label=\\'micro-average ROC curve (area = {0:0.2f})\\'\\n               \\'\\'.format(roc_auc[\"micro\"]),\\n         color=\\'deeppink\\', linestyle=\\':\\', linewidth=4)\\n\\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\\n         label=\\'macro-average ROC curve (area = {0:0.2f})\\'\\n               \\'\\'.format(roc_auc[\"macro\"]),\\n         color=\\'navy\\', linestyle=\\':\\', linewidth=4)\\n\\ncolors = cycle([\\'#00FFFF\\',\\'#C0C0C0\\',\\'#0000FF\\',\\'#808080\\',\\'#0000A0\\',\\'#000000\\',\\'#ADD8E6\\',\\'#FFA500\\',\\'#800080\\',\\'#A52A2A\\',\\'#FFFF00\\',\\'#800000\\',\\'#00FF00\\',\\'#008000\\',\\'#FF00FF\\'])\\nfor i, color in zip(range(n_classes), colors):\\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\\n             label=\\'ROC curve of class {0} (area = {1:0.2f})\\'\\n             \\'\\'.format(i, roc_auc[i]))\\n\\nplt.plot([0, 1], [0, 1], \\'k--\\', lw=2)\\nplt.xlim([0.0, 1.0])\\nplt.ylim([0.0, 1.05])\\nplt.xlabel(\\'False Positive Rate\\')\\nplt.ylabel(\\'True Positive Rate\\')\\nplt.title(\\'Receiver Operating Characteristic (ROC) curve\\')\\nplt.legend(loc=\"lower right\")\\nplt.show()\\nplt.savefig(\\'ROC.jpg\\')\\n   \\n\\n\\n                \\n# Print and save classification report\\nprint(\\'Test F1 Accuracy: \\', f1_score(true_bools, pred_bools,average=\\'micro\\'))\\nprint(\\'Test F1 Macro:\\',  f1_score(true_bools, pred_bools,average=\\'macro\\'))\\nprint(\\'Test Flat Accuracy: \\', accuracy_score(true_bools, pred_bools),\\'\\n\\')\\nprint(\\'Hamming score: {0}\\'.format(hamming_score(np.array(true_bools), np.array(pred_bools))))  \\n\\nclf_report = classification_report(true_bools,pred_bools,target_names=[\\'0\\',\\'1\\',\\'2\\',\\'3\\',\\'4\\',\\'5\\',\\'6\\',\\'7\\',\\'8\\',\\'9\\',\\'10\\',\\'11\\',\\'12\\',\\'13\\',\\'14\\'])\\n#pickle.dump(clf_report, open(\\'classification_report.txt\\',\\'wb\\')) #save report\\nprint(multilabel_confusion_matrix(true_bools,pred_bools))\\n#print(clf_report)\\n\\n\\n'"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}