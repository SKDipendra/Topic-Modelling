{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nimport pandas as pd\nfrom pandas import  DataFrame\nimport numpy as np\nimport json\nimport csv\nfrom transformers import BertTokenizer\nimport tensorflow as tf\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import TensorDataset, random_split\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import get_linear_schedule_with_warmup\nimport random\nimport time\nimport datetime\nimport plotly.express as px\nfrom sklearn.metrics import matthews_corrcoef, confusion_matrix,accuracy_score,f1_score\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn import metrics\nfrom nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.porter import PorterStemmer\nfrom gensim import corpora, models\nimport gensim\nfrom gensim.models import CoherenceModel\n#import matplotlib.pyplot as plt\nimport re\nfrom sklearn.model_selection import GridSearchCV","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function for getting the frames\ndef get_frames(df):\n    #using cuda as our device\n    device = torch.device(\"cuda\")\n\n    #load the model from disk\n    model = pickle.load(open('../input/model-for-article/model_article_pickle.sav', 'rb'))\n    \n    #getting the content \n    test_str_list = df.content.values \n    \n    \n \n    \n\n    # Tokenize all of the sentences and map the tokens to thier word IDs.\n    input_ids = []\n    \n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n    # For every sentence...\n    for st in test_str_list:\n        # `encode` will:\n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        encoded_sent = tokenizer.encode(\n                            st,                      # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                       )\n\n        input_ids.append(encoded_sent)\n\n    # Pad our input tokens\n    input_ids = pad_sequences(input_ids, maxlen=128, \n                              dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n    # Create attention masks\n    attention_masks = []\n\n    # Create a mask of 1s for each token followed by 0s for padding\n    for seq in input_ids:\n        seq_mask = [float(i>0) for i in seq]\n        attention_masks.append(seq_mask) \n\n    # Convert to tensors.\n    prediction_inputs = torch.tensor(input_ids)\n    prediction_masks = torch.tensor(attention_masks)\n   \n\n    # Set the batch size.  \n    batch_size = 32  \n\n    # Create the DataLoader.\n    prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n    prediction_sampler = SequentialSampler(prediction_data)\n    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n    # Put model in evaluation mode\n    model.eval()\n\n    # Tracking variables \n    predictions = []\n\n    # Predict \n    for batch in prediction_dataloader:\n            # Add batch to GPU\n            batch = tuple(t.to(device) for t in batch)\n\n            # Unpack the inputs from our dataloader\n            b_input_ids, b_input_mask= batch\n\n            # Telling the model not to compute or store gradients, saving memory and\n            # speeding up validation\n            with torch.no_grad():  # Forward pass, calculate logit predictions.\n                # This will return the logits rather than the loss because we have\n                # not provided labels.\n                # token_type_ids is the same as the \"segment ids\", which\n                # differentiates sentence 1 and 2 in 2-sentence tasks.\n                # The documentation for this `model` function is here:\n                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n                outputs = model(b_input_ids,\n                                token_type_ids=None,\n                                attention_mask=b_input_mask)\n\n            # Get the \"logits\" output by the model. The \"logits\" are the output\n            # values prior to applying an activation function like the softmax.\n            logits = outputs[0]  # Move logits and labels to CPU\n            logits = logits.detach().cpu().numpy()\n            \n            #print('logits', logits)\n           \n\n            #append the predictions (max value index in logits) and true labelsn(label ids)\n            predictions.extend((np.argmax(logits, axis=1).flatten()))\n    #print('prediction', predictions)\n    return predictions\n        \n   \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This function gets the topics\nclass Topics:\n    # second step: we clean the data\n    def cleanData(doc):\n        texts = []\n        # first step of cleaning data is tokenization\n        tokenizer = RegexpTokenizer(r'\\w+')\n        # cleaning one by one for all the documents\n        for i in doc:\n            # making everything lower case\n            i = str(i)\n            raw = i.lower()\n\n            # converting into tokens\n            tokens = tokenizer.tokenize(raw)\n            # print(tokens)\n            # removing all the stopwords\n            # a list of english stop words\n            en_stop = get_stop_words('en')\n            # extra from my side\n            en_stop.append(\"s\")\n            en_stop.append('mr')\n            en_stop.append('will')\n            en_stop.append('b')\n            # compare our tokens with the list of stopwords above\n            # remove stop words from tokens\n            stopped_tokens = [i for i in tokens if not i in en_stop]\n            # print(stopped_tokens)\n            # now we perform stemming\n            stemmed_tokens = [PorterStemmer().stem(i) for i in stopped_tokens]\n            # the tokens are ready to use for the document matrix now...inserting all the stemmed token into list\n            texts.append(stemmed_tokens)\n            # print('cleaning')\n        print('cleaned')\n        return texts\n\n\n    # The third part is constructing a document term matrix.....(text to word)\n    def constructDocMatrix(texts):\n        # we need to check how frequent a word appears in each document\n        # Dicinoary() iterates through each word in text, giving unique id to them and collects data such as count\n        dictionary = corpora.Dictionary(texts)\n        # now our dictionary must be converted into bag of words\n        # doc2bow converts dictinoary into bag of words\n        corpus = [dictionary.doc2bow(text) for text in texts]\n        print('matrix complete')\n        return corpus, dictionary\n\n\n    # model evaluation; now we calculate the coherence to find the optimum number of k in the doc\n    def checkCoherence(corpus, texts, dictionary, min_k, max_k, interv):\n        coherence = []\n        k_list = []\n        for k in range(min_k, max_k, interv):\n            print('checking')\n            ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, random_state=2, id2word=dictionary, passes=15)\n            coherence_model_lda = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, coherence='c_v')\n            coherence_lda = coherence_model_lda.get_coherence()\n            coherence.append(coherence_lda)\n            k_list.append(k)\n        return coherence, k_list\n\n\n\n\n\n    # find the dominant topic in each document\n    def findDomaninant(ldamodel, corpus, i):\n        topic_weight = []\n        dominant_topic_ = ''\n        # sort the topic weight in descending order and choose the first one i.e highest weight\n        topics = ldamodel[corpus[i]]\n        for topic in topics:\n            topic_weight.append(topic[1])\n            topic_weight.sort(reverse=True)\n        for topic in topics:\n            if topic[1] == topic_weight[0]:\n                dominant_topic_ = topic[0]\n        # print(dominant_topic)\n        topic_weight.clear()\n        return dominant_topic_\n\n    \n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def main():\n    #importing the webis corpus json file \n    df = pd.read_json('../input/webis-json-file/articles_with_adu_liwc_lexicons_content.json', orient='records')\n    #df.set_index('idx', inplace=True)\n    #get frames for 979 editorials\n    frames_list = get_frames(df)\n    df['frame'] = frames_list\n    #print(df['frames'])\n    #get topics for 979 editorials\n    texts = Topics.cleanData(df['content'])\n    corpus, dictionary = Topics.constructDocMatrix(texts)\n    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=40, random_state=2, id2word=dictionary, passes=15)\n    topics = ldamodel.print_topics(num_topics=40, num_words=3)\n    list_topics_graph = []\n    for topic in topics:\n        # print(topic[1])\n        list_topics_graph.append(re.sub('[^a-zA-Z]+', '-', topic[1]))\n    print(list_topics_graph)\n    # findDomaninant(ldamodel, corpus, 0)\n    dominant_topic_list = []\n    dominant_topic = ''\n    #getting the dominant topics for each editorials from the corpus\n    for i in range(len(ldamodel[corpus])):\n        dominant_topic = Topics.findDomaninant(ldamodel, corpus, i)\n        dominant_topic_list.append(dominant_topic)\n    \n    df['topic'] = dominant_topic_list\n    print(df['topic'])\n    df.to_json('new_articles_with_adu_liwc_lexicons_content.json')\n         \n    \n\nif __name__=='__main__':\n    main()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import svm model\nfrom sklearn import svm\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler\nfrom sklearn.svm import SVC  \n\n\ndef svc_param_gridsearch(X, y, nfolds_or_division):\n    Cs = [  0.01, 0.1, 1, 10, 100]\n    param_grid = {'C': Cs, 'class_weight': ['balanced']}#, 'gamma' : gammas}\n    grid_search = GridSearchCV(SVC(kernel='linear'), param_grid, cv=nfolds_or_division, n_jobs=2, scoring='f1_macro')\n    \n    grid_search.fit(X, y)\n    \n    return grid_search.best_params_#, grid_search.best_score_\n\ndef train_save(X_train, y_train, params={}):\n    # 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'\n    print('params ', str(params))\n    cw = params['class_weight'] if 'class_weight' in params else None\n    c = params['C'] if 'C' in params else 'balanced'\n    clf = svm.SVC(kernel='linear', class_weight=cw, C=c)  \n    return clf\n\ndef normalize(X_train, X_test, normalizing_method=\"standard\"):\n    if normalizing_method == \"standard\":\n        print(\"Normalizing by using standard scaler...\")\n        scaler = StandardScaler(copy=True, with_mean=False)\n        scaler.fit(X_train)\n        X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n        X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns) if (X_test is not None) else None\n    elif normalizing_method == \"log\":    \n        X_train = np.log(X_train+1)\n        X_test = np.log(X_test+1) if (X_test is not None) else None\n    elif normalizing_method == \"sqrt\":\n        X_train = np.sqrt(X_train+(2/3))\n        X_test = np.sqrt(X_test+(2/3)) if (X_test is not None) else None\n    elif normalizing_method == \"minmax\":\n        scaler = MinMaxScaler(copy=True)\n        scaler.fit(X_train)\n\n        X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n        X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns) if (X_test is not None) else None\n    else:\n        print(\"data is not normalized. method is not supported. Choose one of: standard, minmax, log, sqrt\")\n\n    return X_train, X_test\n\n#importing the new updated dataframe with topics and frames\ndf1 = pd.read_json('../input/new-webis-json-file/new_articles_with_adu_liwc_lexicons_content(1).json', orient='records')\n\n#print(df1['mpqa_subjobg_subj'])\n#print(df1[\"mpqa_subjobg_subj\"].describe()) \ndf1['mpqa_subjobg_subj'] = df1['mpqa_subjobg_subj'].fillna(0)\n#print(df1.isnull().sum())\n#lists of combination for the best model (liberal)\nliwc, mpqa_sub, mpqa_arg, content = [],[],[],[]\n#style features\nliwc = [l for l in df1.columns if l.startswith('liwc_')]\nmpqa_arg = [l for l in df1.columns if l.startswith('mpqa_arg_')]\nmpqa_sub = [l for l in df1.columns if l.startswith('mpqa_subjobg_')]\nnrc_emo_sent =  [l for l in df1.columns if l.startswith('nrc_')]\nwebis_adu =  [l for l in df1.columns if l.startswith('adu_')]\n#content\ncontent = [l for l in df1.columns if l.startswith('lemma_')]\n#split label\nsplit = [l for l in df1.columns if l.startswith('split')]\n#majority label\nliberal = [l for l in df1.columns if l.startswith('liberal')]\nconservative = [l for l in df1.columns if l.startswith('conservative')]\ntopics = df1.topic.values\nframes = df1.frame.values\n#print(df1.topic.value_counts())\n#print(df1.frame.value_counts())\n\ndf2 = pd.DataFrame()\n#adding all the required columns (combinations) to new dataframe\n#for a in liwc:\n    #df2[a] = df1[a]\nfor b in mpqa_arg:\n    df2[b] = df1[b]\n#for c in mpqa_sub:\n    #df2[c] = df1[c]\nfor d in content:\n    df2[d] = df1[d]\nfor e in split:\n    df2[e] = df1[e]\n#for i in nrc_emo_sent:\n    #df2[i] = df1[i]\n#for j in webis_adu:\n    #df2[j] = df1[j]\n#for f in liberal:\n    #df2[f] = df1[f]\nfor g in conservative:\n    df2[g] = df1[g]\n#df2['topic'] = topics\ndf2['frame'] = frames\n\n#now we create 2 dataframes (for train and test) and add all these features to it\ndf_train = df2.loc[df2['split_label'] == 'train']\ndf_test = df2.loc[df2['split_label'] == 'test']\n#print(df_train.shape)\n#print(df_test.shape)\n\n\n#this will concat 2 dataframes without losing their index\ndf_merge = pd.concat([df_train, df_test])\n#print(df_merge['split_label'][-200:])\n\ntarget = df_merge.conservative_majority.values\ndf_merge = df_merge.drop([ 'split_label','conservative_majority'], axis=1)\n\n\n\n#print(df_merge.isnull().sum())\n\n#now we use sk learn train test split, without suffling and split (783-196) just as in the paper\nX_train, X_test, y_train, y_test = train_test_split(df_merge,target, test_size=196,shuffle = False)\n\n#X_train, X_test = normalize(X_train, X_test, normalizing_method='sqrt')\nbest_params = svc_param_gridsearch(X_train, y_train, nfolds_or_division=5)\n\nclf_trained = train_save(X_train, y_train, params=best_params)\n\n#Train the model using the training sets\nclf_trained.fit(X_train, y_train)\n\n\n#Predict the response for test dataset\ny_pred = clf_trained.predict(X_test)\n\n# Model Accuracy: how often is the classifier correct?\n#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\nprint(f1_score(y_test, y_pred, average='micro'))\nprint(f1_score(y_test, y_pred, average='macro'))\n\n'''fix test_size and nan value in the last column'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}