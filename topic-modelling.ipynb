{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport glob\nfrom nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.porter import PorterStemmer\nfrom gensim import corpora, models\nimport gensim\nfrom gensim.models import CoherenceModel\nimport matplotlib.pyplot as plt\nimport re\nimport numpy as np\nimport pandas as pd","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def importDoc():\n    df = pd.read_csv(\"../input/article-level-dataset-for-frames/corpus_revised3.csv\", header=None, names=['id', 'primary_frame','text'])\n    texts = df.text.values\n    print(texts)\n    return texts\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# second step: we clean the data\ndef cleanData(doc):\n    texts = []\n    # first step of cleaning data is tokenization\n    tokenizer = RegexpTokenizer(r'\\w+')\n    # cleaning one by one for all the documents\n    for i in doc:\n        # making everything lower case\n        raw = i.lower()\n        # converting into tokens\n        tokens = tokenizer.tokenize(raw)\n        # print(tokens)\n        # removing all the stopwords\n        # a list of english stop words\n        en_stop = get_stop_words('en')\n        # extra from my side\n        en_stop.append(\"s\")\n        en_stop.append('mr')\n        en_stop.append('will')\n        en_stop.append('b')\n        # compare our tokens with the list of stopwords above\n        # remove stop words from tokens\n        stopped_tokens = [i for i in tokens if not i in en_stop]\n        # print(stopped_tokens)\n        # now we perform stemming\n        stemmed_tokens = [PorterStemmer().stem(i) for i in stopped_tokens]\n        # the tokens are ready to use for the document matrix now...inserting all the stemmed token into list\n        texts.append(stemmed_tokens)\n    # print(texts)\n    return texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The third part is constructing a document term matrix.....(text to word)\ndef constructDocMatrix(texts):\n    # we need to check how frequent a word appears in each document\n    # Dicinoary() iterates through each word in text, giving unique id to them and collects data such as count\n    dictionary = corpora.Dictionary(texts)\n    # now our dictionary must be converted into bag of words\n    # doc2bow converts dictinoary into bag of words\n    corpus = [dictionary.doc2bow(text) for text in texts]\n    # print(corpus)\n    return corpus, dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model evaluation; now we calculate the coherence to find the optimum number of k in the doc\ndef checkCoherence(corpus, texts, dictionary, min_k, max_k):\n    coherence = []\n    k_list = []\n    for k in range(min_k, max_k):\n        print('checking')\n        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, random_state=2, id2word=dictionary, passes=15)\n        coherence_model_lda = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_lda = coherence_model_lda.get_coherence()\n        coherence.append(coherence_lda)\n        k_list.append(k)\n    return coherence, k_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bar graph for visualization of coherence\ndef makeGraph(topics,doc_no ):\n\n    y_pos = np.arange(len(topics))\n\n\n    plt.barh(y_pos, doc_no, align='center', alpha=0.5)\n    plt.yticks(y_pos, topics)\n    plt.yticks(fontsize=8)\n\n\n    plt.ylabel('Top 3 Terms per Topic')\n    plt.xlabel('Number of Editorials')\n    plt.title('Number of Editorials per Topic')\n\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find the dominant topic in each document\ndef findDomaninant(ldamodel,corpus,i):\n    topic_weight = []\n    #sort the topic weight in descending order and choose the first one i.e highest weight\n    topics = ldamodel[corpus[i]]\n    for topic in topics:\n        topic_weight.append(topic[1])\n        topic_weight.sort(reverse=True)\n    for topic in topics:\n        if topic[1] == topic_weight[0]:\n            dominant_topic = topic[0]\n    #print(dominant_topic)\n    topic_weight.clear()\n    return dominant_topic\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#first, we check the coherence to pick the optimal number of topics\ndoc = importDoc()\ntexts = cleanData(doc)\ncorpus, dictionary = constructDocMatrix(texts)\n# print(corpus)\n# Finally we have the document term matrix (corpus) which we can input in the model\nprint('k vako')\nprint(checkCoherence(corpus, texts, dictionary, 10, 12))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def importDoc():\n    df = pd.read_csv(\"../input/article-level-dataset-for-frames/corpus_revised3.csv\", header=None, names=['id', 'primary_frame','text'])\n    texts = df.text.values\n    #print(texts)\n    return texts\n\n\n\n# second step: we clean the data\ndef cleanData(doc):\n    texts = []\n    # first step of cleaning data is tokenization\n    tokenizer = RegexpTokenizer(r'\\w+')\n    # cleaning one by one for all the documents\n    for i in doc:\n        # making everything lower case\n        raw = i.lower()\n        # converting into tokens\n        tokens = tokenizer.tokenize(raw)\n        # print(tokens)\n        # removing all the stopwords\n        # a list of english stop words\n        en_stop = get_stop_words('en')\n        # extra from my side\n        en_stop.append(\"s\")\n        en_stop.append('mr')\n        en_stop.append('will')\n        en_stop.append('b')\n        # compare our tokens with the list of stopwords above\n        # remove stop words from tokens\n        stopped_tokens = [i for i in tokens if not i in en_stop]\n        # print(stopped_tokens)\n        # now we perform stemming\n        stemmed_tokens = [PorterStemmer().stem(i) for i in stopped_tokens]\n        # the tokens are ready to use for the document matrix now...inserting all the stemmed token into list\n        texts.append(stemmed_tokens)\n        #print('cleaning')\n    print('cleaned')\n    return texts\n\n\n# The third part is constructing a document term matrix.....(text to word)\ndef constructDocMatrix(texts):\n    # we need to check how frequent a word appears in each document\n    # Dicinoary() iterates through each word in text, giving unique id to them and collects data such as count\n    dictionary = corpora.Dictionary(texts)\n    # now our dictionary must be converted into bag of words\n    # doc2bow converts dictinoary into bag of words\n    corpus = [dictionary.doc2bow(text) for text in texts]\n    print('matrix complete')\n    return corpus, dictionary\n\n\n# model evaluation; now we calculate the coherence to find the optimum number of k in the doc\ndef checkCoherence(corpus, texts, dictionary, min_k, max_k, interv):\n    coherence = []\n    k_list = []\n    for k in range(min_k, max_k, interv):\n        print('checking')\n        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, random_state=2, id2word=dictionary, passes=15)\n        coherence_model_lda = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_lda = coherence_model_lda.get_coherence()\n        coherence.append(coherence_lda)\n        k_list.append(k)\n    return coherence, k_list\n\n\n# bar graph for visualization of coherence\ndef makeGraph(topics,doc_no ):\n\n    y_pos = np.arange(len(topics))\n\n\n    plt.barh(y_pos, doc_no, align='center', alpha=0.5)\n    plt.yticks(y_pos, topics)\n    plt.yticks(fontsize=8)\n\n\n    plt.ylabel('Top 3 Terms per Topic')\n    plt.xlabel('Number of Editorials')\n    plt.title('Number of Editorials per Topic')\n\n\n    plt.show()\n\n\n#find the dominant topic in each document\ndef findDomaninant(ldamodel,corpus,i):\n    topic_weight = []\n    #sort the topic weight in descending order and choose the first one i.e highest weight\n    topics = ldamodel[corpus[i]]\n    for topic in topics:\n        topic_weight.append(topic[1])\n        topic_weight.sort(reverse=True)\n    for topic in topics:\n        if topic[1] == topic_weight[0]:\n            dominant_topic = topic[0]\n    #print(dominant_topic)\n    topic_weight.clear()\n    return dominant_topic\n\n\n\ndoc = importDoc()\nprint('1')\ntexts = cleanData(doc)\nprint('2')\ncorpus, dictionary = constructDocMatrix(texts)\n# print(corpus)\n# Finally we have the document term matrix (corpus) which we can input in the model\nprint('3')\n#print(checkCoherence(corpus, texts, dictionary,40,50,2))\n# Applying model\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=44, random_state=2, id2word=dictionary, passes=15)\ntopics = ldamodel.print_topics(num_topics=44, num_words=3)\nlist_topics_graph = []\nfor topic in topics:\n    #print(topic[1])\n    list_topics_graph.append(re.sub('[^a-zA-Z]+', '-', topic[1]))\nprint(list_topics_graph)\n#findDomaninant(ldamodel, corpus, 0)\ndoc_list = []\ncorpus_first = []\ndoc_count = 0\ndoc_count_list = []\nfor topic in topics:\n    for i in range(len(ldamodel[corpus])):\n        dominant_topic = findDomaninant(ldamodel, corpus, i)\n        #print('dominant topic', dominant_topic)\n        #print('topic[0]', topic[0])\n        if topic[0] == dominant_topic:\n            doc_count = doc_count + 1\n    #print(doc_count)\n    doc_count_list.append(doc_count)\n    doc_count = 0\nprint(doc_count_list)\nmakeGraph(list_topics_graph, doc_count_list)\n","execution_count":null,"outputs":[{"output_type":"stream","text":"1\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}